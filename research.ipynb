{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from ActiveLearning import GAL, GNN\n",
    "import numpy.linalg as nla\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_mat(X):\n",
    "\tX = X / nla.norm(X, axis=-1).reshape(-1, 1)\n",
    "\tcos_sim_mat = (X @ X.T) - np.eye(X.shape[0])\n",
    "\tcos_sim_mat = np.absolute(cos_sim_mat)\n",
    "\treturn cos_sim_mat\n",
    "\n",
    "def construct_graph(X, thresh=.8):\n",
    "\tcos_sim_mat = sim_mat(X)\n",
    "\tedges = np.vstack(np.where(cos_sim_mat > thresh))\n",
    "\treturn edges\n",
    "\n",
    "def GNN_embed(X, edges, gnn):\n",
    "\treturn gnn(X, edges)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_q1.pkl', 'rb') as f:\n",
    "\tdataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "train_samples, train_labels = dataset['train_samples'], dataset['train_labels']\n",
    "test_samples, test_labels = dataset['test_samples'], dataset['test_labels']\n",
    "available_pool_samples, available_pool_labels = dataset['available_pool_samples'][:K], dataset['available_pool_labels'][:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(*available_pool_samples.T, c=available_pool_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = kmeans.fit(available_pool_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(available_pool_samples).min(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 40\n",
    "budget_per_iter = 50\n",
    "train_limit = 2010\n",
    "gal = GAL(\n",
    "\tdataset,\n",
    "\tNone,\n",
    "\titerations,\n",
    "\tbudget_per_iter,\n",
    "\ttrain_limit,\n",
    "\t.9,\n",
    "\tNone,\n",
    "\tLogisticRegression,\n",
    "\tGNN\n",
    ")\n",
    "# gnn = GNN(3, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, gnn_model = gal._train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = gal.sim_mat(gal.train_samples)\n",
    "G = gal.construct_graph(A, gal.available_pool_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gal.entropy(gal.available_pool_samples, model)) == len(gal.available_pool_samples)\n",
    "# gal.density_score(gal.available_pool_samples)\n",
    "# gal.entropy(gal.available_pool_samples, model)\n",
    "# nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009117808967824611"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# for n in range(1, 11):\n",
    "# \tcoef_vector = np.random.beta(1, [1/n, 1/n, n], size=(3))\n",
    "# \tprint(coef_vector)\n",
    "\n",
    "# np.random.beta(1, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_idx = gal.select_points(G, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = gal.sim_mat(available_pool_samples)\n",
    "E = gal.construct_graph(A, available_pool_samples)\n",
    "# list(zip(E))\n",
    "# list(zip(*E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(train_samples, train_labels)\n",
    "\n",
    "# E[0].nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "\tif not isinstance(X, torch.Tensor):\n",
    "\t\tX = torch.Tensor(X)\n",
    "\tENT = (X * torch.log2(X)).sum(dim=-1)\n",
    "\treturn ((ENT - ENT.min()) / (ENT.max() - ENT.min())).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(model.predict_proba(available_pool_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENT_DICT = dict(zip(range(len(available_pool_samples)), entropy(model.predict_proba(available_pool_samples))))\n",
    "PR_DICT = nx.pagerank(E[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {}\n",
    "for k in ENT_DICT.keys():\n",
    "\ts[k] = ENT_DICT[k] + PR_DICT[k] \n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.pagerank(E[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_samples = np.concatenate([train_samples, available_pool_samples], axis=0)\n",
    "D_labels = np.concatenate([train_labels, available_pool_labels], axis=0)\n",
    "gnn_labeled_idx = list(range(len(train_samples)))\n",
    "\n",
    "A = gal.sim_metric(D_samples)\n",
    "D_gnn, E_gnn = gal.construct_graph(A, D_samples)\n",
    "\n",
    "D_samples_torch = torch.Tensor(D_samples)\n",
    "E_gnn_torch = torch.tensor(E_gnn)\n",
    "D_embed = gnn(D_samples_torch, E_gnn_torch)\n",
    "# D_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3)\n",
    "x_emb = torch.rand(2, 5)\n",
    "print(x, x_emb)\n",
    "torch.cat([x, x_emb], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = available_pool_samples\n",
    "A = gal.sim_mat(V)\n",
    "\n",
    "G, E = gal.construct_graph(A, V)\n",
    "\n",
    "V = torch.Tensor(V)\n",
    "E = torch.tensor(E)\n",
    "\n",
    "U_idx = gal.select_points(G)\n",
    "gnn_labeled_idx.extend([e + len(train_samples) for e in U_idx])\n",
    "\n",
    "gal.label_update(U_idx)\n",
    "gnn.train(gnn_labeled_idx)\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_labeled_idx.extend([e + len(train_samples) for e in U_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_labeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn(V, E)[U_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "U_idx = gal.select_points(G)\n",
    "# V_emb = gnn(V, E)[U_idx]\n",
    "gal.label_update(U_idx)\n",
    "# gal._train_model()\n",
    "\n",
    "\n",
    "# R = gal.unceartinty_score(G)\n",
    "# R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAT = [\n",
    "\t[0, 1, 0],\n",
    "\t[1, 0, 0],\n",
    "\t[0, -2, 0]\n",
    "]\n",
    "\n",
    "pairwise_distances(MAT, MAT)\n",
    "# sim_mat(MAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances(available_pool_samples[:100], available_pool_samples[:100], metric='cosine')\n",
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat(train_samples).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct_graph(train_samples)\n",
    "# train_samples\n",
    "G = nx.Graph()\n",
    "for i, e in enumerate(gal.train_samples):\n",
    "\tG.add_node(i, label=','.join(map(lambda x: str(round(x, 2)), e)))\n",
    "\n",
    "edges = sim_mat(train_samples)\n",
    "edges = np.vstack(np.where(edges > .8))\n",
    "\n",
    "for e in edges.T:\n",
    "\tG.add_edge(*e)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "# nx.degree_centrality(G)\n",
    "R = nx.pagerank(G)\n",
    "# print(R)\n",
    "D = sorted(R, key=lambda x: R[x], reverse=True)[:5]\n",
    "R, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(model, 'forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model).__contains__('forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callable(model.register_backward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_means = [(7, 3), (1, 1), (6, 10)]\n",
    "ppc = 30\n",
    "X = np.vstack([np.random.normal(c, np.random.random(), size=(ppc, 2)) for c in cluster_means])\n",
    "X = torch.tensor(X).type(torch.float)\n",
    "Y = torch.cat([(torch.ones(ppc) * i) for i in range(len(cluster_means))]).reshape(-1, 1).type(torch.long)\n",
    "# print(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=Y.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(X)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "\ttotal_loss = 0\n",
    "\tfor x, y in zip(X, Y):\n",
    "\t\n",
    "\t\to = model(x.unsqueeze(dim=0))\n",
    "\n",
    "\t\tloss = criterion(o, y)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\tprint(f'[{epoch + 1} / {EPOCHS}]: loss - {total_loss / len(X)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=model.predict(X).argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from ActiveLearning import ActiveLearningPipeline, generate_plot\n",
    "\n",
    "with open('dataset_q1.pkl', 'rb') as f:\n",
    "\tdataset = pickle.load(f)\n",
    "\n",
    "iterations = 40\n",
    "budget_per_iter = 50\n",
    "train_limit = 2010\n",
    "selection_criteria = ['random', 'custom']\n",
    "accuracy_scores_dict = defaultdict(list)\n",
    "for criterion in selection_criteria:\n",
    "\tAL_class = ActiveLearningPipeline(dataset=dataset,\n",
    "\t\t\t\t\t\t\t\t\tselection_criterion=criterion,\n",
    "\t\t\t\t\t\t\t\t\titerations=iterations,\n",
    "\t\t\t\t\t\t\t\t\tbudget_per_iter=budget_per_iter,\n",
    "\t\t\t\t\t\t\t\t\ttrain_limit=train_limit)\n",
    "\taccuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
    "generate_plot(accuracy_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as nla\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(dataset['available_pool_samples'])\n",
    "Y = dataset['available_pool_labels']\n",
    "\n",
    "plt.scatter(X[:100, 0], X[:100, 1], c=Y[:100])\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = dataset['available_pool_samples'][:100]\n",
    "A = A / nla.norm(A, axis=-1).reshape(-1, 1)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim_mat = D @ D.T\n",
    "cos_sim_mat = (A @ A.T) - np.eye(A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sim = np.sort(cos_sim_mat, axis=1)[:, -2::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .8\n",
    "edges = np.where(cos_sim_mat > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
